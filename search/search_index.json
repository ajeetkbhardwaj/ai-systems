{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"aiagents/ch1intro/","title":"AI Agents - Introduction","text":""},{"location":"dlops/ch1intro/","title":"Deep Learning - From Developement to Production","text":""},{"location":"dlops/pytorchddp/","title":"Pytorchddp","text":"<p>Why Distributed Training of Models ? 1. Saves time 2. Increase the amount of compute 3. Helps to train model faster</p> <p>As model became large and larger over time we was't able to fit it one single GPU machine thus we need the distributed training via model parallel and FSDB</p> <p>Start non-distributed training job Scaling-up 1. Multiple GPU on one machine 2. Multiple machine on cluster End deploying a training job across several machines in a cluster. How DDP can we use to train our LLM model faster ?</p> <p>How DDP works under the hood</p> <p>What is DistributedSampler</p> <p>How gradients are synchronized across GPUs</p> <p>What is procedure of model training on non-distributed systems  ?</p> <p> Start with  1. Model takes InputBatch of Data from DataLoader 2. Pass it to forward pass to model to calculate the error(loss) and backward pass to compute parameters of gradients which optimizer uses to update the model.</p> <p>What is procedure of model training on distributed systems ? 0. Distributing training jobs to 4 GPU 1. DDP launches one process per GPU 2. each process has one local copy of model, each parameters, optimizers etc remains same on each process of model 3. DDP internally maintains the synchronization throughout the training process 4. each GPU has same model, we change the data such that each process takes different data input from input batch from dataloader  To do it, we use the DistributedSampler to distribute the equal amount of different data from same input batch to different GPU instances from model training. So, we are processing 4 time a data compared to single GPU  </p> <p>Feed the input to each model run the forward and backward pass to compute loss and gradient they are different on each GPU because data is different. When we run the optimizer, it will result the different parameters at same time across GPU devices which endup with distinct model instead of the single model.  Thus, DDP initiate the synchronization step, gradient from all replicas are aggreageted using Bucketed All reduced algorithms(It overlaps the gradient computation with communication)  means as we run the forward pass algorithms communicates with gradients on each machines which ensures that each machine running properly and at the end each model will have same replica.  Now we can run the optimizer on each instance of model for training it.</p> <p>What is Distributed Data Parallel ?</p> <p>PyTorch DistributedDataParallel (DDP) which enables data parallel training in PyTorch. Data parallelism is a way to process multiple data batches across multiple devices simultaneously to achieve better performance. In PyTorch, the DistributedSampler ensures each device gets a non-overlapping input batch. The model is replicated on all the devices; each replica calculates gradients and simultaneously synchronizes with the others using the ring all-reduce algorithm.</p> <p></p> <p>How to migrate a single-GPU training script to multi-GPU via DDP</p> <p>Setting up the distributed process group</p> <p>Saving and loading models in a distributed setup how to use DDP in code.  We will start with a single-GPU training script and migrate that to running it on 4 GPUs on a single node.</p>"},{"location":"hardware/ch1intro/","title":"Introduction","text":"<p>Artificial Intelligence (AI) systems hardware refers to the specialized and general-purpose computing components designed to enable and accelerate AI tasks such as model training, inference, and data processing. AI workloads often require intensive computation, parallel processing, and fast access to large datasets.</p>"},{"location":"hardware/ch1intro/#key-components-of-ai-systems-hardware","title":"Key Components of AI Systems Hardware:","text":"<ul> <li>Central Processing Units (CPUs): General-purpose processors that handle a wide range of tasks, including AI control flow, data preprocessing, and running less complex AI models. CPUs excel at sequential processing but are less efficient than other hardware for large-scale deep learning.</li> <li>Graphics Processing Units (GPUs): Specialized for highly parallel operations, GPUs accelerate AI model training and inference, especially for deep learning involving vast matrix operations. They are widely adopted in AI research and applications due to their scalability and performance.</li> <li>Tensor Processing Units (TPUs): Custom AI accelerators developed by Google to optimize tensor operations common in deep learning. TPUs offer high efficiency and power for tasks like large-scale model training and are primarily accessible via cloud platforms.</li> <li>Field-Programmable Gate Arrays (FPGAs): Reprogrammable hardware that balances performance and flexibility, ideal for low-latency, real-time AI applications such as autonomous vehicles or trading systems.</li> <li>Application-Specific Integrated Circuits (ASICs): Custom-designed chips optimized for specific AI tasks. ASICs provide maximal efficiency and power savings but lack flexibility and require high development costs, suitable for large-scale or embedded AI deployments.</li> <li>Memory and Storage: Large amounts of RAM are crucial for handling AI data during computation, especially with large datasets and complex models. Fast storage solutions like NVMe SSDs enable rapid data access and processing.</li> <li>Networking: High-speed communication infrastructure is essential for distributed AI systems, facilitating data and computation sharing across clusters or edge devices and cloud servers.</li> </ul>"},{"location":"hardware/ch1intro/#considerations-based-on-ai-application","title":"Considerations Based on AI Application:","text":"<ul> <li>Deep Learning: Requires powerful GPUs or TPUs, large RAM, and fast storage to manage large datasets and complex model training.</li> <li>Edge AI: Needs energy-efficient, compact hardware like FPGAs or custom chips tailored for limited-resource devices.</li> <li>High-Performance Computing (HPC) AI: Uses clusters of CPUs, GPUs, and TPUs optimized for large-scale computations and research tasks.</li> </ul>"},{"location":"hardware/ch1intro/#summary-table-of-ai-hardware-types","title":"Summary Table of AI Hardware Types:","text":"Hardware Type Strengths Limitations Best Use Cases CPUs Versatile, good for sequential tasks, affordable Less efficient at parallel AI tasks, slower training General purpose AI, preprocessing GPUs Excellent parallel processing, scalable, widely supported High energy and cost Deep learning training, simulations TPUs Optimized for AI tensor operations, power-efficient Less flexible, mostly cloud-based Large-scale deep learning on cloud FPGAs Customizable, low latency, energy-efficient Complex programming, lower raw performance Real-time AI, specialized low-latency tasks ASICs Maximum performance and power efficiency High development cost, inflexible Large-scale specialized AI, embedded AI <p>AI hardware plays a foundational role in enabling AI algorithms and models to process data quickly and effectively, making it possible to deploy advanced AI applications across industries. Choosing the right hardware depends on the specific requirements of the AI workload, including scale, complexity, latency, and power constraints.</p> <p>This overview provides a solid introduction to AI systems hardware and its critical importance in building and scaling AI solutions.</p> <p>Here is a clear, summarized explanation based on your input and supported by authoritative sources:</p>"},{"location":"hardware/ch1intro/#what-is-artificial-intelligence","title":"What is Artificial Intelligence?","text":"<p>John McCarthy, the father of AI, defined artificial intelligence as: \u201cThe science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable.\u201d</p> <p>In essence, AI is about creating systems that can perform tasks requiring human-like intelligence such as learning, reasoning, problem-solving, and decision-making.</p>"},{"location":"hardware/ch1intro/#deep-learning-vs-machine-learning","title":"Deep Learning vs. Machine Learning","text":"<ul> <li>Deep Learning: Automates much of the feature extraction process, meaning the system automatically discovers the representations needed for detection or classification from raw data, minimizing human intervention. It uses neural networks with multiple layers to model complex patterns.</li> <li>Classical Machine Learning: Typically requires more human guidance to select and design features from data, relying on algorithms that learn from these features to perform tasks.</li> </ul>"},{"location":"hardware/ch1intro/#factors-driving-ai-advancement","title":"Factors Driving AI Advancement","text":"<p>The growth and success of AI have been powered by the following:</p> <ul> <li>Algorithmic Innovation: New models, architectures, and methods that improve AI capabilities.</li> <li>Data: Availability of large datasets, both supervised (labeled) and interactive environments for training AI models.</li> <li>Compute Power: Increasing computational resources to train complex models efficiently, including GPUs, TPUs, and distributed computing systems.</li> </ul> <p>This foundational understanding connects AI\u2019s conceptual origins to practical modern advances in the field.</p> <p> </p> <p>The computational needs for AI algorithms vary depending on the type, complexity, and scale of the tasks involved. Here are key points about the computation requirements for AI algorithms:</p>"},{"location":"hardware/ch1intro/#computation-needs-for-ai-algorithms","title":"Computation Needs for AI Algorithms","text":"<ul> <li>High Computational Cost: AI algorithms, especially deep learning models, are computationally intensive. Training these models involves processing large datasets with millions or billions of parameters through numerous mathematical operations, often requiring powerful and specialized hardware like GPUs, TPUs, or large clusters of CPUs.</li> <li>Time Complexity: The amount of time taken by an AI algorithm to run depends on the algorithm design and input data size. Deep learning training can take hours, days, or even weeks depending on model complexity and hardware efficiency.</li> <li>Space Complexity: AI algorithms require significant memory storage during training and inference for data, intermediate calculations, and model parameters. This demands high RAM capacity and fast storage solutions like NVMe SSDs.</li> <li>Parallel Processing: Many AI algorithms benefit from parallel computing. GPUs and TPUs are designed to perform many operations simultaneously, drastically reducing training and inference time compared to traditional CPUs.</li> <li>Iterative Training: AI models are typically trained iteratively, requiring repeated passes over large data to optimize the algorithm. This repetitive computation multiplies total compute needs.</li> <li> <p>Algorithm Specific Requirements:</p> </li> <li> <p>Deep Learning Neural Networks: Very high compute and memory needs due to multilayer architectures and backpropagation training.</p> </li> <li>Natural Language Processing (NLP): Needs large compute resources to process and understand extensive language data and context.</li> <li>Classical Machine Learning: Generally less computationally demanding but still needs adequate compute for training on large datasets.</li> <li>Compute Scaling: As models and datasets grow, computational resource demands scale up, necessitating advances in hardware accelerator technology and distributed computing infrastructure.</li> </ul>"},{"location":"hardware/ch1intro/#summary","title":"Summary","text":"<p>AI algorithms require substantial computational resources characterized by the need for fast processing, large memory, and efficient parallelism. Hardware accelerators like GPUs, TPUs, and FPGAs play a critical role in meeting these demands, enabling feasible training and real-time inference of complex AI models.</p> <p>This computational intensity is a defining characteristic of AI development and influences all stages from model training to deployment and real-world application.</p>"},{"location":"hardware/ch1intro/#introduction-to-computing-systems","title":"Introduction to Computing Systems","text":"<p>A computing system is a combination of hardware and software components that work together to process, store, and communicate information. It functions to execute instructions and perform tasks ranging from simple calculations to complex operations.</p>"},{"location":"hardware/ch1intro/#key-components-of-a-computing-system","title":"Key Components of a Computing System:","text":"<ul> <li>Input Unit:The interface through which data and instructions enter the system from external sources. It converts this input into machine-readable form. Common input devices include keyboards, mice, scanners, and sensors.</li> <li> <p>Central Processing Unit (CPU):Often called the brain of the computer, the CPU carries out instructions by performing arithmetic and logical operations, controlling data flow, and managing communication among components. It contains:</p> </li> <li> <p>Arithmetic Logic Unit (ALU): Executes mathematical and logical operations.</p> </li> <li>Control Unit (CU): Directs operations of the system by controlling data flow and instruction execution.</li> <li>Registers: Small, fast memory locations for temporary storage of data and instructions.</li> <li> <p>Memory/Storage Unit:This unit stores data and instructions.</p> </li> <li> <p>Primary Memory (RAM): Temporary storage for active data and programs.</p> </li> <li>Secondary Storage: Persistent storage such as hard drives, SSDs, and cloud storage.</li> <li>Output Unit:Converts processed data into a human-understandable form. Includes monitors, printers, speakers, and other devices that deliver results to users.</li> <li>System Bus:   A set of wires or pathways that facilitate data transfer among CPU, memory, and input/output units. It includes the data bus, address bus, and control bus, each serving distinct communication purposes.</li> </ul>"},{"location":"hardware/ch1intro/#computing-system-architecture","title":"Computing System Architecture","text":"<p>Most modern computing systems follow the Von Neumann architecture, which organizes the system into CPU, memory, input, and output, connected via a common bus. Data and instructions are stored together in memory and accessed sequentially for processing.</p>"},{"location":"hardware/ch1intro/#summary_1","title":"Summary","text":"<p>Computing systems integrate multiple functional components to process data efficiently:</p> Component Function Input Unit Acquires and converts input data CPU Processes instructions, performs calculations Memory Unit Stores data temporarily or permanently Output Unit Presents processed results to users System Bus Interconnects components for communication <p>Understanding these essentials helps grasp how computing devices operate and how hardware and software jointly facilitate digital processing.</p>"},{"location":"hardware/ch1intro/#traditional-computing-systems-overview","title":"Traditional Computing Systems Overview","text":"<p>Traditional computing systems consist of hardware and software components designed to perform three fundamental tasks: Computation, Communication, and Storage/Memory. These systems follow hierarchical memory and processor organization to optimize speed, capacity, and performance.</p>"},{"location":"hardware/ch1intro/#core-functions-in-traditional-computing-systems","title":"Core Functions in Traditional Computing Systems","text":"<ul> <li>Computation:Performed by the Central Processing Unit (CPU), which executes instructions, performs arithmetic and logical operations, and controls data flow.</li> <li>Communication:Data exchange between CPU, memory, storage, and input/output devices through system buses and interconnects.</li> <li>Storage/Memory:   Hierarchical structure for storing instructions and data temporarily or permanently with varying speeds and sizes.</li> </ul>"},{"location":"hardware/ch1intro/#modern-memory-hierarchy-in-traditional-computing-systems","title":"Modern Memory Hierarchy in Traditional Computing Systems","text":"Memory Type Size Access Time Characteristics &amp; Management Register File ~32 words Sub-nanosecond Small, ultra-fast storage close to CPU. Used for current instruction operands. L1 Cache Tens of KB Nanoseconds (ns) First level cache, very fast, manually or compiler managed register spilling. L2 Cache Hundreds of KB to Few MB Few nanoseconds Larger than L1, slower, hardware automatically manages caching. L3 Cache Several MBs More nanoseconds Larger and slower than L2, shared among cores in multicore CPUs. Main Memory (DRAM) Several GBs ~100 nanoseconds Bulk memory, slower than caches, holds programs/data during execution. Swap Disk Hundreds of GB to TB Tens of microseconds to milliseconds Slowest memory level, extends physical memory by disk storage, managed by operating system through automatic demand paging."},{"location":"hardware/ch1intro/#key-points-on-memory-management","title":"Key Points on Memory Management","text":"<ul> <li>Registers are managed manually by programmers or compilers.</li> <li>Caches (L1, L2, L3) are automatically managed by hardware for efficient data access and minimizing latency.</li> <li>Main memory (DRAM) provides large capacity but slower access compared to caches.</li> <li>Swap space on disks is used as overflow when main memory is full. This management happens automatically via demand paging controlled by the operating system.</li> </ul>"},{"location":"hardware/ch1intro/#summary_2","title":"Summary","text":"<p>Traditional computing systems utilize a memory hierarchy from fastest and smallest (registers) to slowest and largest (disk swap), balancing the trade-off between speed and capacity. Efficient memory management ensures high performance during computation and data processing.</p> <p>This layered memory approach, combined with well-orchestrated computation and communication across components, forms the backbone of traditional computer architecture systems.</p>"},{"location":"hardware/ch1intro/#key-trends-in-modern-ai-computing-and-memory-systems","title":"Key Trends in Modern AI Computing and Memory Systems","text":"<ul> <li>Data Access Bottleneck:Accessing data is a major limiting factor in AI computations. AI algorithms consume vast amounts of data, making data movement and access speed critical to performance.</li> <li>Data-Hungry AI Algorithms:Modern AI, especially deep learning, requires processing enormous datasets during both training and inference, pushing hardware and memory systems to their limits.</li> <li>Energy Consumption:Energy efficiency is a key challenge, as moving data\u2014particularly between off-chip and on-chip memory\u2014consumes more energy than the computation itself.</li> <li>Data Movement Energy Cost:   The energy cost of transferring data from off-chip memory (DRAM or storage) to on-chip memory (cache or local memory within processing units) dominates overall power consumption.</li> </ul>"},{"location":"hardware/ch1intro/#modern-memory-systems-cerebras-wafer-scale-engine-wse","title":"Modern Memory Systems: Cerebras Wafer Scale Engine (WSE)","text":"<p>Cerebras has developed revolutionary wafer-scale AI accelerators addressing these challenges:</p> Feature Cerebras WSE (2019) Cerebras WSE-2 (2021) Comparison: Largest GPU (NVIDIA Ampere GA100) Transistor Count 1.2 Trillion 2.6 Trillion 54.2 Billion Chip Area 46,225 mm\u00b2 46,225 mm\u00b2 826 mm\u00b2 Number of Cores 400,000 850,000 \u2014 On-Chip Memory 18 GB 40 GB \u2014 Memory Bandwidth 9 Petabytes/second 20 Petabytes/second \u2014 <p>Highlights:</p> <ul> <li>The Cerebras WSE chips are wafer-scale processors, meaning the entire silicon wafer is a single chip, significantly larger than traditional GPU chips.</li> <li>Massive on-chip memory and extremely high memory bandwidth (up to 20 PB/s) dramatically reduce the data movement bottleneck.</li> <li>The architecture integrates hundreds of thousands of cores tightly coupled with on-chip memory, reducing energy and latency costs associated with off-chip memory access.</li> <li>By contrast, even the largest GPUs have far fewer transistors and much smaller memory bandwidth, highlighting Cerebras\u2019 innovations in supporting extreme-scale AI workloads.</li> </ul>"},{"location":"hardware/ch1intro/#summary_3","title":"Summary","text":"<p>Modern AI hardware like the Cerebras Wafer Scale Engine tackles critical challenges including data access bottlenecks and energy consumption by radically scaling chip size, core count, and on-chip memory, providing massive memory bandwidth to keep up with the demands of data-hungry AI algorithms. This design minimizes costly data movement and boosts the overall efficiency and speed of AI model training and inference.</p> <p>These advancements point toward a future where larger, faster, and more energy-efficient AI computing platforms enable ever more complex and capable AI systems.</p>"},{"location":"hardware/ch1intro/#specialized-computation-engines","title":"Specialized Computation Engines","text":""},{"location":"hardware/ch1intro/#specialized-computation-engines-and-the-end-of-performance-free-lunch","title":"Specialized Computation Engines and the End of Performance 'Free Lunch'","text":"<p>Herb Sutter's landmark 2005 article, \"The free lunch is over: A Fundamental Turn Toward Concurrency,\" highlights a critical shift in computing hardware and software development caused by hitting physical limits in processor speed improvements.</p>"},{"location":"hardware/ch1intro/#key-points-from-herb-sutters-the-free-lunch-is-over","title":"Key Points from Herb Sutter\u2019s \"The Free Lunch is Over\"","text":"<ul> <li>End of Exponential Clock Speed Increases:Historically, CPU clock speeds increased exponentially (from MHz to GHz) delivering significant single-thread performance gains. Around mid-2000s, this trend halted at about 3-4 GHz due to physical and power constraints.</li> <li>Need for Faster Applications:Despite the stagnation in clock speeds, application demands for higher performance continue to grow, implying that traditional methods to speed up software execution are no longer sufficient.</li> <li>Shift to Parallel Computing:The solution lies in increasing computational density through parallelism: dividing tasks into smaller subtasks and processing these simultaneously on multiple processor cores or specialized accelerators.</li> <li>Software and Hardware Paradigm Shift:   Modern processors incorporate multiple cores rather than faster single cores, supported by accelerators designed for specialized tasks (e.g., GPUs, TPUs). This hardware shift necessitates concurrent software development to leverage parallelism effectively.</li> </ul>"},{"location":"hardware/ch1intro/#conceptual-summary","title":"Conceptual Summary:","text":"Aspect Before Free Lunch Over After Free Lunch Over Processor Speed Growth Exponential (MHz to GHz increase) Saturated at ~3-4 GHz (plateau) Performance Improvement Mostly sequential, single-threaded Parallel processing, multi-core &amp; accelerators Software Model Mostly sequential programming Requires concurrency and parallelism Task Execution Single thread, linear execution Concurrent subtasks executed simultaneously Hardware Design Single-core CPU focus Multi-core CPUs and specialized accelerators"},{"location":"hardware/ch1intro/#implications-for-specialized-computation-engines","title":"Implications for Specialized Computation Engines","text":"<ul> <li>Specialized accelerators (e.g., AI chips) are examples of hardware designed to exploit task-specific parallelism to break through the performance ceiling.</li> <li>Parallelizing workloads allows tasks to be split and processed by many cores or accelerators, maximizing computational throughput despite clock speed stagnation.</li> <li>The challenge is both hardware (creating many-core or accelerator chips) and software (developing concurrent applications that effectively utilize parallel hardware).</li> </ul> <p>In summary, the \"free lunch\" of automatic performance improvement from increasing clock speeds is over. Achieving high computational density now depends on parallel computing, multi-core processors, and specialized accelerators, requiring a fundamental shift in both computer architecture and programming paradigms.</p>"},{"location":"hardware/ch1intro/#evolution-of-nvidia-gpus-v100-vs-a100","title":"Evolution of NVIDIA GPUs: V100 vs A100","text":"<p>NVIDIA GPUs have evolved significantly to meet the demands of high-performance computing and AI workloads. Here\u2019s a comparison highlighting key architectural differences between the Tesla V100 and the Ampere A100 GPUs:</p>"},{"location":"hardware/ch1intro/#nvidia-tesla-v100-volta-architecture-2017","title":"NVIDIA Tesla V100 (Volta Architecture, 2017)","text":"<ul> <li>Stream Processors: 5120</li> <li>NVIDIA Terminology: SIMT execution (Single Instruction, Multiple Threads)</li> <li>Generic Terms:</li> <li>80 cores</li> <li>64 SIMD (Single Instruction, Multiple Data) functional units per core</li> <li>Tensor Cores: First generation tensor cores specifically designed to accelerate deep learning workloads.</li> <li>Performance:</li> <li>15.7 TFLOPS (Single Precision)</li> <li>7.8 TFLOPS (Double Precision)</li> <li>125 TFLOPS (Deep Learning Tensor operations)</li> </ul>"},{"location":"hardware/ch1intro/#nvidia-a100-ampere-architecture-2020-2021","title":"NVIDIA A100 (Ampere Architecture, 2020-2021)","text":"<ul> <li>Stream Processors: 6912</li> <li>NVIDIA Terminology: SIMT execution</li> <li>Generic Terms:</li> <li>108 cores</li> <li>64 SIMD functional units per core</li> <li>Tensor Cores: Improved tensor cores with enhanced capabilities for mixed precision and sparsity support, accelerating AI model training and inference.</li> <li>Additional Features:</li> <li>Support for sparsity (sparse matrix operations boosting efficiency)</li> <li>New floating-point data type (Tensor Float 32 / TF32) for better performance with minimal precision loss.</li> <li>Performance:</li> <li>19.5 TFLOPS (Single Precision)</li> <li>9.7 TFLOPS (Double Precision)</li> <li>312 TFLOPS (Deep Learning Tensor operations)</li> </ul>"},{"location":"hardware/ch1intro/#key-architectural-improvements-from-v100-to-a100","title":"Key Architectural Improvements from V100 to A100","text":"<ul> <li>Increased core count and stream processors, driving higher parallelism.</li> <li>Enhanced tensor core design, making AI training and inference more efficient and faster.</li> <li>Support for sparsity and new numerical formats (TF32), enabling faster computation with optimized accuracy.</li> <li>Significant uplift in raw performance across precision levels, with over 2.5x increase in deep learning TFLOPS.</li> </ul>"},{"location":"hardware/ch1intro/#summary-table","title":"Summary Table","text":"Feature Tesla V100 (Volta) A100 (Ampere) Stream Processors 5120 6912 Cores (Generic) 80 108 SIMD Units per Core 64 64 Tensor Cores 1st Gen Improved Gen with sparsity Single Precision TFLOPS 15.7 19.5 Double Precision TFLOPS 7.8 9.7 Deep Learning TFLOPS (Tensor) 125 312 New Features - TF32, sparsity support <p>These GPUs demonstrate the rapid evolution of NVIDIA\u2019s architectures, focusing heavily on AI workloads with increasingly specialized hardware such as tensor cores and advanced precision formats to accelerate deep learning at scale.</p> <p>For a deeper dive, official NVIDIA whitepapers and developer blogs provide detailed technical insights on these architectures.</p>"},{"location":"hardware/ch1intro/#overview-of-gpu-based-accelerators","title":"Overview of GPU-Based Accelerators","text":"<ul> <li>Purpose: GPUs were initially designed for graphics rendering but became widely adopted for general-purpose computations, especially for training and inference of neural networks.</li> <li>Architecture:</li> <li>Consist of thousands of small cores optimized for parallel processing (SIMT - Single Instruction, Multiple Threads).</li> <li>Excels at large-scale matrix and vector operations, enabling high throughput.</li> <li>Strengths:</li> <li>High raw computational power.</li> <li>Well-supported ecosystem with mature software frameworks.</li> <li>Efficient for training large, complex neural networks.</li> <li>Limitations:</li> <li>High power consumption.</li> <li>Less efficient for low-latency, custom, and small-batch inference tasks.</li> </ul>"},{"location":"hardware/ch1intro/#overview-of-fpga-based-accelerators","title":"Overview of FPGA-Based Accelerators","text":"<ul> <li>Purpose: Field Programmable Gate Arrays (FPGAs) are reprogrammable silicon chips that can be customized to implement specific hardware logic, including neural network operations.</li> <li>Architecture:</li> <li>Consist of a flexible array of logic blocks and interconnects that can be configured for different algorithms.</li> <li>Lower-level hardware customization enables tailored designs optimized for specific workloads.</li> <li>Strengths:</li> <li>High adaptability and reconfigurability.</li> <li>Lower power consumption and latency compared to GPUs.</li> <li>Useful for real-time and embedded AI applications.</li> <li>Limitations:</li> <li>Generally lower raw computational throughput than GPUs.</li> <li>Programming and optimization require specialized expertise.</li> <li>Higher upfront development cost compared to off-the-shelf GPUs.</li> </ul>"},{"location":"hardware/ch1intro/#typical-fpga-based-accelerator-use-cases","title":"Typical FPGA-Based Accelerator Use Cases","text":"<ul> <li>Real-time AI inference in embedded systems.</li> <li>Energy-efficient AI processing where power constraints are critical.</li> <li>Custom AI workloads requiring specialized data types or computation flows.</li> <li>Prototyping and hardware exploration before ASIC design.</li> </ul>"},{"location":"hardware/ch1intro/#market-share-as-of-q3-2017","title":"Market Share (As of Q3 2017)","text":"Technology Leading Vendors Market Share GPU NVIDIA (72.8%), AMD Dominant for AI training and general-purpose acceleration FPGA Xilinx (53%), Altera (36%), Microsemi (7%), Lattice Semiconductor (3%) Preferred for customized, low-power, and specialized AI tasks"},{"location":"hardware/ch1intro/#the-gap-and-trade-offs","title":"The Gap and Trade-offs","text":"<ul> <li>GPUs offer higher peak performance and are easier to program with existing AI frameworks, making them ideal for large-scale training and cloud-based AI.</li> <li>FPGAs outperform GPUs in specific AI inference workloads where efficiency, latency, and power consumption are critical.</li> <li>The choice depends on application needs\u2014scalability and general-purpose power vs. customizability and efficiency.</li> </ul> <p>This comparison highlights that while GPUs remain the leading choice for AI training, FPGAs play a vital and complementary role in specialized AI acceleration tasks. Each technology offers unique benefits tailored to particular use cases in the AI ecosystem.</p> <p>[1] https://jaewoong.org/pubs/fpt16-accelerating-bnn.pdf [2] https://sse.uni-hildesheim.de/media/fb4/informatik/AG_SSE/Christopher_Noel_Hesse.pdf [3] https://arxiv.org/html/2502.02304v1 [4] https://www.ibm.com/think/topics/fpga-vs-gpu [5] https://www.sciencedirect.com/science/article/pii/S1877050923014175 [6] https://nicsefc.ee.tsinghua.edu.cn/networkone.html</p> <p>[1] https://deepgram.com/learn/evolution-of-gpu [2] https://www.youtube.com/watch?v=as-aVVm9JZI [3] https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units [4] https://www.nvidia.com/en-in/technologies/ [5] https://www.nvidia.com/en-in/about-nvidia/corporate-timeline/ [6] https://wolfadvancedtechnology.com/nvidia-gpu-architecture/ [7] https://en.wikipedia.org/wiki/Category:Nvidia_microarchitectures [8] https://www.slideshare.net/slideshow/nvidia-history-gpu-architecture-and-new-pascal-architecture/77659363</p> <p>[1] http://www.gotw.ca/resources/Software%20and%20Concurrency%20-%20OGDC.pdf [2] https://cppdepend.com/blog/is-the-free-lunch-over-revisiting-herb-sutter-prediction/ [3] https://www.cs.utexas.edu/~lin/cs380p/Free_Lunch.pdf [4] https://www.sqrlab.ca/courses/csci4060u-w25/CSCI_4060U_Lecture_01.pdf [5] https://herbsutter.com/welcome-to-the-jungle/ [6] https://www.bibsonomy.org/bibtex/11540da30b9424bf778012a96f1192730/gron [7] https://herbsutter.com/2011/12/29/welcome-to-the-jungle/ [8] https://queue.acm.org/detail.cfm?id=1095421</p> <p>[1] https://8968533.fs1.hubspotusercontent-na1.net/hubfs/8968533/Cerebras%20Wafer%20Scale%20Cluster%20datasheet%20-%20final.pdf [2] https://arxiv.org/html/2503.11698v1 [3] https://hc34.hotchips.org/assets/program/conference/day2/Machine%20Learning/HC2022_Cerebras_Final_v02.pdf [4] https://sdk.cerebras.net/computing-with-cerebras [5] https://www.cerebras.ai/blog/cerebras-wafer-scale-engine-outperforms-nvidia-h100-in-carbon-capture-simulations [6] https://www.cerebras.ai/chip [7] https://www.cerebras.ai/blog/announcing-the-cerebras-architecture-for-extreme-scale-ai [8] https://www.ddn.com/resources/reference-architectures/cerebras-wafer-scale-clusters/</p> <p>[1] https://vardhaman.org/wp-content/uploads/2021/03/CO.pdf [2] https://www.geeksforgeeks.org/cloud-computing/architecture-of-cloud-computing/ [3] https://www.geeksforgeeks.org/difference-between-cloud-computing-and-traditional-computing/ [4] https://en.wikipedia.org/wiki/Computer_architecture [5] https://www.aztechit.co.uk/blog/cloud-computing-vs-traditional [6] https://estuary.dev/blog/distributed-architecture/ [7] https://www.scribd.com/document/854266499/Traditional-Computing [8] https://unstop.com/blog/cloud-computing-architecture</p> <p>[1] https://www.tutorialspoint.com/computer-system-architecture [2] https://bgibhopal.com/what-are-the-fundamental-components-and-architecture-of-a-computer-system/ [3] https://www.geeksforgeeks.org/functional-components-of-a-computer/ [4] https://vardhaman.org/wp-content/uploads/2021/03/CO.pdf [5] https://onlineamrita.com/blog/computer-architecture-fundamentals-and-components/ [6] https://www.geeksforgeeks.org/computer-organization-architecture/computer-and-its-components/ [7] https://www.mbit.edu.in/wp-content/uploads/2020/05/computer-systems-Architecture.pdf [8] https://en.wikipedia.org/wiki/Computer_architecture [9] https://www.ccbp.in/blog/articles/basic-organization-of-computer-system</p> <p>[1] https://www.larksuite.com/en_us/topics/ai-glossary/computational-complexity-of-common-ai-algorithms [2] https://cloudian.com/guides/ai-infrastructure/6-types-of-ai-workloads-challenges-and-critical-best-practices/ [3] https://www.geeksforgeeks.org/artificial-intelligence/hardware-requirements-for-artificial-intelligence/ [4] https://www.proxpc.com/blogs/system-requirements-for-artificial-intelligence-in-2025 [5] https://www.geeksforgeeks.org/artificial-intelligence/ai-algorithms/ [6] https://dev.to/ajaytie/the-building-blocks-of-ai-algorithms-data-and-computing-power-2nj3 [7] https://www.tableau.com/data-insights/ai/algorithms [8] https://ainowinstitute.org/publications/compute-and-ai [9] https://www.tierpoint.com/blog/ai-workloads/</p> <p>[1] http://www.incompleteideas.net/papers/Sutton-JAGI-2020.pdf [2] https://www.vbspu.ac.in/e-content/Sunil-Kumar/ARTIFICIAL%20INTELLIGENCE.pdf [3] https://www.teneo.ai/blog/homage-to-john-mccarthy-the-father-of-artificial-intelligence-ai [4] https://www.uio.no/studier/emner/matnat/ifi/IN5480/h21/Grupper/gruppe-4/idaode_iteration1.pdf [5] https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist) [6] https://hai-production.s3.amazonaws.com/files/2020-09/AI-Definitions-HAI.pdf [7] http://jmc.stanford.edu/artificial-intelligence/what-is-ai/ [8] https://www.ibm.com/think/topics/artificial-intelligence [9] https://link.springer.com/chapter/10.1007/978-3-031-56713-1_1 [10] https://www-formal.stanford.edu/jmc/whatisai.pdf</p> <p>[1] https://www.geeksforgeeks.org/artificial-intelligence/hardware-requirements-for-artificial-intelligence/ [2] https://www.intel.com/content/www/us/en/learn/ai-hardware.html [3] https://www.ibm.com/think/topics/ai-hardware [4] https://www.youtube.com/watch?v=-BFuQi6ff7Y [5] http://elearn.psgcas.ac.in/nptel/courses/video/106106238/L02.html [6] https://www.scale4me.org/scale-curriculum/introduction [7] https://onlinecourses.nptel.ac.in/noc25_cs98/preview [8] https://www.sciencedirect.com/science/article/abs/pii/S0065245820300541 [9] https://www.youtube.com/watch?v=08qwI9RX8-M</p>"},{"location":"hardware/ch2processors/","title":"Processors","text":"<p>Hi Process</p>"},{"location":"llmops/art-1-llmops-nptel/","title":"MLOps for LLMs","text":"<p>Introduction to operations for Large Language Models(LLMs) also called as MLOps for LLMs.</p> <p>Objectives</p> <ul> <li>Model Versioning for base models and fine-tuned variants, CI/CD specifics</li> <li>Accuracy, Performance vs Cost tradeoffs, Observability, Security &amp; Governance (Bias, Toxicity, Explainability)</li> </ul> <p></p>"},{"location":"llmops/art-1-llmops-nptel/#understanding-llms","title":"Understanding LLMs","text":"<p>An LLM is defined as an advanced AI model trained on language datasets, which are primarily text data.</p> <p>The model is engineered to predict the next token of text based on the current context captured in a \"context window\"</p> <p>LLMs accept various forms of \"multimodal input\" such as text, images, audio, and video, which are collectively called \"prompts\"</p> <p>To construct a response, an LLM can use several tools, including Browse the web, executing programs, and accessing a database or data store</p>"},{"location":"llmops/art-1-llmops-nptel/#llm-building-process","title":"LLM Building Process","text":"<p>The process of building LLMs involves three main stages: Pre-training (Stage 1): This involves training the model on massive datasets, such as data from the worldwide web, to create a foundational model. This stage is performed by large companies and typically happens on a yearly basis due to the scale and cost involved.</p> <p>Fine-tuning (Stage 2): In this stage, a pre-trained foundational model is fine-tuned for specific tasks, such as creating a model for the insurance domain. The fine-tuned models can be produced more frequently, on a weekly or monthly basis.</p> <p>Usage (Stage 3): This is the final stage where the LLM is used as an API, receiving prompts and providing programmatic responses, such as JSON outputs.</p> <p>the capabilities of LLMs are widely publicized, but here our focus will be on operational aspects of working with them, which happens around the LLM. </p> <p>The characteristics of classical machine learning (ML) models, deep learning models, and large language models (LLMs) across several key factors.</p> <p>Training Time: Classical ML is fast, deep learning is slower, and LLMs, particularly during pre-training, take the longest, often weeks on GPU clusters.</p> <p>Model Size: Classical ML models are small, deep learning models are large, and LLMs are extremely large. The speaker notes that this is a key factor in the distinction between proprietary and open models.</p> <p>Data and Compute: LLMs demand \"web-scale\" datasets and require significant GPU support, unlike classical and deep learning models which have lesser requirements.</p> <p>Task Flexibility: Classical ML models are task-specific, deep learning models have some generalization, but LLMs are known for their \"general-purpose nature\" and wide applicability, allowing them to perform various tasks like generating stories or code.</p> <p>Explainability: Classical ML is the most explainable, deep learning has less explainability, and LLMs have \"absolutely no explainability\" beyond token prediction probabilities.</p> <p>Inference Speed: Classical ML is very fast, deep learning is relatively fast, but LLMs are \"significantly slow\" in comparison, even with optimizations.</p> <p>Security Risks: The video highlights that LLMs have a significantly larger \"attack surface\" with new risks such as prompt hijacking and data leakage, compared to classical and deep learning models.</p> <p>Transferability: Classical ML has poor transferability, deep learning has moderate transferability, but LLMs have \"high transferability\" and can perform zero-shot or few-shot learning on new contexts with minimal guidance.</p> <p>What is the difference between Open Source vs Open Weights LLM ?</p> Feature Open-Weight LLM Open-Source LLM What is shared? Only the pre-trained modelweights (parameters) are made public. The modelweights, source code, training algorithms, and often the training data are all publicly available. Transparency Low. You can see the final, trained model but not the \"recipe\" used to create it. It's like receiving a compiled program without the source code. High. The entire process is transparent, allowing anyone to inspect, modify, and understand how the model was built and trained. Reproducibility Limited. Without access to the training data and code, you cannot fully reproduce the model from scratch. You can only verify its performance. High. The entire training process can be replicated by others, which is vital for scientific research and ethical oversight. Modification You can fine-tune the model for specific tasks, but you cannot fundamentally change its architecture or retrain it from scratch using the original methodology. You have full control. You can modify the architecture, change the training algorithms, and adapt every aspect of the model to your needs. Community The community can use and fine-tune the model, but their ability to contribute to the core development is limited. A vibrant community can contribute to every aspect of the project, from bug fixes and new features to documentation and ethical reviews. Licensing Often released under licenses that are permissive (e.g., Apache 2.0) but may have restrictions, such as commercial use limitations or requirements for attribution. Adheres to traditional open-source principles (e.g., MIT, Apache 2.0), granting broad freedoms for use, modification, and redistribution."},{"location":"llmops/art-1-llmops-nptel/#key-challenges-of-llms","title":"Key Challenges of LLMs","text":"<p>What are the key challenges in ML, DL and LLMs ?</p> <p></p> <p>The challenges of LLMs compared to classical machine learning, breaking them down by the three stages of LLM interaction: pre-training, fine-tuning, and usage.</p>"},{"location":"llmops/art-1-llmops-nptel/#1-pre-training-stage","title":"1. Pre-training Stage","text":"<ul> <li>Data challenges: The process requires terabytes of diverse, high-quality text data, which is a significant and costly exercise that only a handful of large companies can undertake.</li> <li>Computational resources: Pre-training demands powerful, specialized hardware like GPU clusters and parallel training techniques to process massive datasets.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#2-fine-tuning-stage","title":"2. Fine-tuning Stage","text":"<ul> <li>Control and Interpretability: A key challenge is controlling the fine-tuned model's specific outputs without losing its inherent power to generalize to similar tasks with minimal guidance. It's difficult to guarantee that a fine-tuned model will produce explainable outputs or work on affiliated tasks.</li> <li>Continuous Updates: Fine-tuned models can become outdated quickly because the foundation models they are based on have a \"view of the world as of a particular date\".</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#3-usage-stage","title":"3. Usage Stage","text":"<ul> <li>Latency: While human-scale latencies may seem acceptable, LLM inference speeds are significantly slower than classical ML models and can become a major issue when integrated into applications that require real-time processing.</li> <li>Hallucination Risk: Unlike classical ML's misclassification, LLMs can generate wrong but confidently stated answers, making it difficult to detect without external fact-checkers or verifiers.</li> <li>Model Size: LLMs are very large, which is why managed LLM services are popular and expensive.</li> <li>Security and Abuse: LLMs face new and numerous security risks, such as prompt injection and data leakage, which are far more prevalent than in classical ML pipelines .</li> <li>Ethical and Bias Concerns: The biases in LLMs are learned from the pre-training datasets, making them deeply embedded. Detecting and mitigating these biases requires addressing the issue at the data collection stage itself.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#pre-traning-data-challenges","title":"Pre-Traning : Data Challenges","text":"<p>Note : Key operational items related to data and compute in the pretraining stage</p> <p>There are alots of challenges involved in the pre-training stage of Large Language Models (LLMs), focusing primarily on data and compute</p> <ul> <li>Low-quality or noisy data : We know that LLMs rely on vast amounts of web data for pre-training, this data is often low-quality, noisy, and contains duplicates. Incorrect data can lead to the model learning fake relationships, toxicity, and biases, or even hallucinating facts. Thus we needed to mitigate such thing from the data using The mitigation strategy involving aggressive data cleaning, deduplication, filtering, and quality checks grounded in human-verifiable labels etc.</li> <li>Biases and toxicity : The data can contain embedded biases and toxicity, which, if not addressed, can worsen the problem and make the bias an inherent part of the model. To mitigate this, bias detection and toxicity filtering must be incorporated into the data sourcing pipeline from the beginning, often with a human feedback loop.</li> <li>Data imbalance : Web data is predominantly in the English language and has a strong leaning toward American English contexts, which can lead to data imbalance. This is problematic because half the world's population resides in regions like Southeast Asia and China, where social media usage is high, but this data is often not indexed for LLM pre-training. As a result, models trained on this data may not be proficient in other languages. For example, the video mentions a model assuming women in the medical field are nurses rather than doctors.</li> <li>Leakage of Sensitive Information : LLMs can inadvertently train on sensitive information, such as personal identifiable information (PII), from public repositories. This could result in intellectual property lawsuits or massive fines under regulations like GDPR. The mitigation strategy involves using PII scrubbing tools and carefully selecting curated data sources.</li> <li>Outdated or Stale Data : It's a challenge to keep foundation models updated with new information since retraining them is time-consuming and expensive. Retrieval-Augmented Generation (RAG) can help with this issue.</li> <li>Engineering Challenges : The video also covers the engineering bottleneck of processing data at a large scale. Data ingress can be very slow, causing GPU clusters to be idle while waiting for data . To mitigate this, big data engineering techniques and synthetic data can be used.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#pre-training-compute-challenges","title":"Pre-Training : Compute Challenges","text":"<p>  What are the challenges related to the compute resources during the pre-training stage of Large Language Models (LLMs).</p> <ul> <li>Memory Bottlenecks: The large model sizes and batch sizes can exceed the memory of a single GPU, leading to out-of-memory errors in a distributed cluster setup. To mitigate this, frequent checkpointing and compressed floating-point representations (using 16-bit instead of 32-bit) are recommended.</li> <li>Checkpointing and Failure Recovery: Checkpointing is highlighted as a critical practice to avoid significant loss of progress. The speaker emphasizes the need to save progress frequently to reliable, persistent storage so training can be resumed from the last checkpoint in case of failures.</li> <li>Compute Inefficiency: The  suboptimal data pipelines can lead to idle GPU time. This often happens when code is written in a way that doesn't leverage the distributed nature of the cluster. The solution is to use proper big data principles like data and model parallelism, and to optimize data loaders and I/O processes.</li> <li>Network Bandwidth: The distributed nature of LLM training means that multi-node communication can be a bottleneck. The training process is only as fast as the slowest node, so high-bandwidth, low-latency connections between machines are crucial.</li> <li>Hyperparameter Sensitivity: Incorrect hyperparameters, such as learning rates, can lead to wasted computational resources if the model fails to converge effectively. It is recommended to follow best practices and start with conservative settings before scaling up.</li> <li>Storage Bottlenecks: The massive datasets used in pre-training can cause storage issues. A mix of hybrid storage options, like using SSDs for frequent checkpoints and regular disks for other data, is suggested to prevent GPUs from being starved of data.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#versioning-in-llms","title":"Versioning in LLMs","text":"<p>Versioning of pretrained and finetuned LLMs, adapting git-flow to LLM development, and testing</p> <p>How to apply the mlops techniques like data and model versioning, testing, CI/CD etc in LLM or do we needed some changes for need of LLM ?</p> <p>or</p> <p>how to apply best practices from classical machine learning (ML) to large language models (LLMs), focusing on versioning, CI/CD, and testing.</p> <p> </p>"},{"location":"llmops/art-1-llmops-nptel/#versioning-for-foundation-models","title":"Versioning for Foundation Models :","text":"<p>These models are long-lived and multi-purpose, used across different teams and contexts.</p> <ul> <li>Versioning for these is similar to stable main branches in Git repositories.</li> <li>Data versioning for foundation models needs to be multi-modal, supporting text, video, audio, and images.</li> <li>Traditional change detection methods like modification time or checksums are not effective for non-textual data because small changes can be registered as an entirely new file.</li> <li>Given the massive size of the datasets and the extensive planning required for pre-training, the speaker recommends that dedicated data versioning tools may not be necessary. Instead, the lineage, checkpointing, and meticulous data curation process for these models should provide sufficient traceability.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#versioning-for-fine-tuned-models","title":"Versioning for Fine-Tuned Models","text":"<ul> <li>Fine-tuned models are task-specific and require a model-centric versioning approach.</li> <li>Unlike classical ML, where the focus is on versioning the code, it makes more sense to version the models themselves due to their large size.</li> <li>A critical aspect of fine-tuning is the regular update of data. This updated data is incremental and represents a regular data versioning exercise that can be performed.</li> <li>The versioning process for fine-tuned models involves versioning the entire model, including its lineage. This ensures a clear link between data updates, code changes, and the new fine-tuned model versions that are produced.</li> <li>The speaker provides a potential scenario where a base foundational model is fine-tuned weekly, with additional optimizations and new varieties being produced over time. All these variations result in new model versions.</li> <li>Tools like DVC (Data Version Control) are still useful in this context. The speaker also suggests adapting the Gitflow paradigm for ML projects to be used with fine-tuned models.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#adapting-git-flow-to-finetuned-models","title":"Adapting git-flow to finetuned models","text":"<p>how to adapt the GitFlow paradigm for fine-tuned Large Language Models (LLMs), a process that differs significantly from classical machine learning (ML) projects.</p> <ul> <li>Applicability : GitFlow is not recommended for foundational models because they are rarely updated. However, it is highly applicable for fine-tuned models.</li> <li>Artifact Size : For classical ML, the primary artifacts are code, which is typically in megabytes. For LLMs, the models themselves are large, so this must be considered when using GitFlow.</li> <li>Tooling : DVC (Data Version Control) can be used to checkpoint and version the large fine-tuned models, including their weights and hyperparameters. The code itself is managed in a standard Git repository.</li> <li>Release Cycles :</li> <li>Classical ML uses feature-driven releases, allowing for easy rollbacks to a stable branch.</li> <li>LLMs require continuous fine-tuning, and the cost of retraining is high. As a result, rolling back might be very expensive or impractical.</li> <li>Storage : While classical ML stores code in a repository (local or remote), LLMs use a combination of different storage systems. Code is stored in a Git repo, but the large models might be stored in dedicated model registries, which often have their own storage systems.</li> <li>Branching :</li> <li>Classical ML uses separate branches for features and bug fixes.</li> <li>For LLMs, additional branches may be created for new experiments like distillation, quantization, or LoRA (Low-Rank Adaptation).</li> <li>Versioning :</li> <li>In classical ML, version tags are attached to each release.</li> <li>For LLMs, it's more useful to attach semantic model tags, which helps in tracking which tag corresponds to which model.</li> <li>Rollback Strategy :</li> <li>GitFlow allows for easy rollback by simply checking out a previous release branch.</li> <li>For LLMs, a full redeployment might be necessary, and you may need to manage checkpoints yourself.</li> <li>Governance : In addition to standard governance practices like code reviews and CI/CD, LLM fine-tuning requires human-in-the-loop audits for bias checking, testing, and dataset tracking.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#fine-tuning-testing","title":"Fine-Tuning : Testing","text":"<p>What are the challenges and methods for testing Large Language Models (LLMs) ?</p>"},{"location":"llmops/art-1-llmops-nptel/#challenges-of-testing-llms","title":"Challenges of Testing LLMs","text":"<ul> <li>Nondeterminism: Unlike traditional software, which produces consistent outputs, LLMs have inherent noise and variability. This means they can produce different answers for the same prompt, making it difficult to use deterministic testing methods.</li> <li>Testing Creativity: The process of an LLM predicting the next token and generating varied responses is similar to human creative thinking, making it difficult to test in a conventional, deterministic way.</li> <li>Evolving Performance: An anecdote is shared about GPT-3.5, which initially showed strong capabilities in areas like math and code generation but then regressed in performance on those same tasks in a later version. This highlights the importance of regression tests.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#recommended-testing-practices","title":"Recommended Testing Practices","text":"<ul> <li>Data Validation: It's essential to validate data used for fine-tuning models to ensure quality and prevent data drift .</li> <li>Model Performance Tests: For factual or numerical data, you can measure accuracy by comparing the LLM's output against known correct answers.</li> <li>Regression Tests: It is crucial to perform regression tests to ensure that new versions of a model do not perform worse on tasks that previous versions excelled at.</li> <li>Online Testing: Unlike software, which can be certified in a CI cycle and be expected to perform the same in production, LLMs need continuous online testing in a production environment.</li> <li>Tests should screen for issues like hallucination, toxicity, and consistency in real-time as the model generates responses.</li> <li>This is important because prompts from end-users can cause the model to behave unpredictably, a scenario that may not have been captured in the CI stage.</li> <li>CI/CD is a must: The speaker emphasizes that Continuous Integration and Continuous Deployment (CI/CD) is essential for LLM fine-tuning.</li> </ul> <p>Our focus will be on various testing practices essential for LLMs, adapting the concepts from classical ML and software testing.</p>"},{"location":"llmops/art-1-llmops-nptel/#types-of-tests-for-llms","title":"Types of Tests for LLMs","text":"<ul> <li>Unit Tests: LLMs require specific tests for things like prompt templating and tokenization to ensure the prompts are structured correctly and text is being chunked properly.</li> <li>Data Tests: These are crucial for ensuring data quality and preventing issues like bias, underrepresentation, or overrepresentation . The speaker emphasizes the need to check for biases and maintain data balance.</li> <li>Performance Tests: It is possible to measure traditional metrics like accuracy, F1 score, and recall for LLMs on factual questions where a correct answer is known.</li> <li>Drift Tests: These are important for detecting both data drift and, more uniquely to LLMs, \"semantic drift,\" which refers to changes in the meaning of the responses over time.</li> <li>Security Tests: The speaker notes that security testing is vital for LLMs, as the open nature of language models presents a much larger attack surface compared to structured languages.</li> <li>Qualitative Tests: These are essential for evaluating the model's helpfulness and safety. The speaker provides an example of a chatbot providing a factually correct but completely unhelpful answer, stressing the importance of the model meeting the user's intent. He also mentions the need to test for safety to ensure the model does not provide dangerous instructions.</li> </ul> <p>Note : In LLM full automation might not be possible, and a \"human in the loop\" is necessary to provide feedback for testing.</p>"},{"location":"llmops/art-1-llmops-nptel/#ci-for-llms","title":"CI for LLMs","text":"<p>  the specialized Continuous Integration (CI) process for Large Language Models (LLMs) and compares it to classical machine learning (ML).</p>"},{"location":"llmops/art-1-llmops-nptel/#specialized-infrastructure-for-llms-ci","title":"Specialized Infrastructure for LLMs CI","text":"<ul> <li>LLMs require specialized infrastructure, such as GPUs or TPUs, due to the different type of compute needed.</li> <li>Model registries are necessary to store and version large models, and artifact stores are much larger than typical feature stores.</li> <li>The infrastructure must be able to efficiently load checkpoints and have access to SSDs for faster loading.</li> <li>Evaluation runners need to be able to sample outputs at scale to verify accuracy, consistency, and check for hallucinations, all as part of the CI stage itself, which is a significant change from classical ML.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#adjusting-reproducibility-expectations","title":"Adjusting Reproducibility Expectations","text":"<ul> <li>Unlike classical ML, where retraining a model yields nearly identical outputs, LLMs can have significant output deviations from slight changes in prompt templates, data sets, or hyperparameters.</li> <li>To mitigate this, it's important to track points of randomness and set the random state to a constant value for testing.</li> <li>Tracking checkpoints and hyperparameter sensitivity is also crucial, and it's necessary to accept some degree of tolerance for output variations.</li> <li>The method for modeling the output during testing should be domain-specific.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#prompt-engineering-in-the-ci-lifecycle","title":"Prompt Engineering in the CI Lifecycle","text":"<ul> <li>Prompt engineering is a critical interface for LLMs and must be an integral part of the CI cycle.</li> <li>Prompts should be treated like code, meaning they need to be versioned and compared across different fine-tuning iterations.</li> <li>The CI process should include testing for prompt syntax correctness, verifying output format consistency, and detecting prompt drift.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#artifact-management","title":"Artifact Management","text":"<ul> <li>Due to the large size of fine-tuned models and other artifacts, the CI process needs to be integrated with model registries and cloud repositories.</li> <li>It's important to efficiently cache these artifacts to support multiple jobs.</li> <li>The CI process must also track the lineage from the fine-tuned model back to the base model, including where the base model was downloaded from and how it was set up.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#cd-for-llm-models","title":"CD for LLM Models","text":"<p>how Continuous Deployment (CD) can be applied to large foundation models.</p>"},{"location":"llmops/art-1-llmops-nptel/#cd-for-foundation-models","title":"CD for Foundation Models","text":"<p>Foundation Models: For foundation models, which are like core software components with downstream dependencies, the CD process should prioritize creating stable, safety-critical releases. The git flow main branch is recommended as a suitable vehicle for disseminating these models.</p>"},{"location":"llmops/art-1-llmops-nptel/#cd-for-finetuned-models","title":"CD for Finetuned Models","text":"<p> Fine-tuned Models: Fine-tuned models, which are built on top of foundation models, are released more frequently. Here, the focus of the CD should be on speed and iteration, similar to conventional software development.</p> <p>Gitflow: The video suggests using a Gitflow technique adapted for LLMs. The main branch is used for foundation models, while the release branch is used for fine-tuned models. Experimental fine-tuning variations can be worked on in separate ephemeral branches before being merged back into the release branch via a merge or pull request</p> <p>CI for LLMs, CD for foundation and finetuned models, Typical LLM usage patterns</p>"},{"location":"llmops/art-1-llmops-nptel/#usage-of-deployed-models","title":"Usage of Deployed Models","text":"<p>A common misconception is that Large Language Models (LLMs) learn and update themselves as users interact with them. However, this is not the case. The core weights of the model are not updated during a conversation; instead, the LLM uses a context window to store and retrieve new information provided by the user.</p>"},{"location":"llmops/art-1-llmops-nptel/#why-llms-cant-update-themselves-on-the-fly","title":"Why LLMs Can't Update Themselves on the Fly","text":"<ul> <li>Frozen Weights : LLMs are pre-trained and their weights are frozen during inference. They do not have an internal mechanism for self-improvement or learning from new interactions in real time.</li> <li>Fine-Tuning Cycle : To incorporate new data, an LLM must go through a complete fine-tuning cycle, which is a resource-intensive and time-consuming process.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#what-llms-can-do-with-new-data-online-operations","title":"What LLMs Can Do with New Data (Online Operations)","text":"<p>While LLMs cannot update their core weights on the fly, they can still perform several \"online operations\" by leveraging the information in their context window:</p> <ul> <li>Pattern Recognition : LLMs can spot trends and extrapolate from new examples given within a few-shot context.</li> <li>Reasoning with Examples : They can use a few-shot examples to gain context and perform reasoning tasks.</li> <li>Predicting Trends : To a limited extent, they can predict trends as long as the information fits within the context window, allowing for generalization.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#llm-usage-lookup-orchestration-is-common","title":"LLM Usage : Lookup + Orchestration is common","text":"<p> how to augment the limitations of LLMs when working with new data by using common patterns for data lookup and orchestration.</p>"},{"location":"llmops/art-1-llmops-nptel/#data-lookup-and-orchestration","title":"Data Lookup and Orchestration","text":"<ul> <li>Data Pipeline Integration : An LLM-powered shopping agent needs real-time inventory data, which changes too frequently to be part of the model's fine-tuning cycle. The correct approach is to integrate the LLM's tooling capabilities with a data pipeline that connects to the inventory database. This makes the LLM usage a facet of a broader software product that includes data lookup from a database.</li> <li>Chunking Prompts and Responses : To effectively retrieve information from large documents, like an insurance policy, the document needs to be \"chunked up\" and stored in a vector database for Retrieval Augmented Generation (RAG).</li> <li>Context Assembly : This is a data engineering step where various components are assembled to create a single context for the LLM to answer a prompt. These components include the context window, the user query (prompt), instructions from prompt templates, and data chunks from RAG or other lookups.</li> <li>Anonymization and Masking : This is a necessary step because prompts and responses can accidentally include personally identifiable information (PII). Appropriate engineering is required to handle this based on the context.</li> <li>Conclusion that using an LLM is similar to using any other API, but with specific considerations. The CI/CD pipeline for a product using LLMs must account for factors like response variation and the need for integrations across various data sources through processes like chunking, tokenization, and assembly.</li> </ul>"},{"location":"llmops/art-1-llmops-nptel/#example-customer-service-agent","title":"Example : Customer Service Agent","text":"<p> How to use an LLM as a customer service agent for a bank.</p> <p>How an LLM-powered agent could answer a customer's question, such as why a specific fee was charged. To do this, the agent needs to access several types of information:</p> <ul> <li>Authentication: The agent must first confirm it is speaking with the correct customer to prevent unauthorized information access.</li> <li>Customer Data: The agent needs to fetch the customer's recent interaction data from a database.</li> <li>Bank Policies: It also needs to access bank product information and policies for resolving customer questions</li> </ul> <p>how these different information sources are integrated into the system:</p> <ul> <li>An ID module is exposed as an API that the LLM uses for authentication.</li> <li>The LLM must be able to generate and execute SQL queries to fetch customer interaction data from a database.</li> <li>Bank product and policy documents (like PDFs) are processed using a Retrieval-Augmented Generation (RAG) approach. The documents are chunked and made available for retrieval when needed.</li> </ul> <p>In conclusion that building a system with an LLM agent requires integrating all these components like a regular software system.</p>"},{"location":"llmops/art-1-llmops-nptel/#building-fine-tuned-model","title":"Building Fine-tuned model","text":"<ol> <li>Data processing - data and it's format is right.</li> <li>LLM</li> <li>CI/CD aspect of both pre-training and finetuning</li> <li>After in Production, The usage of LLM in Inferencing</li> <li>Considering all the aspects of the running LLM in production    Building a fine-tuned model and focusing on data processing to ensure quality and proper formatting for LLM architectures</li> </ol> <p>CI/CD (Continuous Integration/Continuous Deployment) aspects for LLMs, specifically the patterns related to pre-training and fine-tuning before an LLM is deployed for inferencing</p>"},{"location":"llmops/art-1-llmops-nptel/#llm-in-production-measuring-llm-accuracy","title":"LLM in Production : Measuring LLM Accuracy","text":"<p>What are the best practices and emerging topics in the field </p> <p>What are the consideration for running LLMs in a production environment and it is the most a critical aspect for a comprehensive understanding of LLM ops</p> <p>Measuring Accuracy : The accuracy as one of the most important attributes of an LLM that defines its usefulness. Accuracy is measured by whether the model's responses are usable, correct, and relevant to the task. Unlike classical machine learning, where accuracy is mathematically verifiable, LLMs have a natural language interface that introduces variability and uncertainty.</p> <p>Three Types of LLM Evaluations To understand and measure accuracy, three types of evaluations:</p> <p>Benchmarks: You can use either general-purpose or domain-specific benchmarks to test the model's performance.</p> <p>Human Evaluation: A human reviews the LLM's output to verify if it meets expectations or is factually accurate.</p> <p>LLM as a Judge: Other LLMs can be used to evaluate and critique the output of the current model.</p>"},{"location":"llmops/art-1-llmops-nptel/#lm-evaluation-harness","title":"LM Evaluation Harness","text":""},{"location":"llmops/art-1-llmops-nptel/#imporving-fine-tuned-llm-accuracy","title":"Imporving Fine-tuned LLM Accuracy","text":"<p>Improving LLM Accuracy There are four effective ways to improve the accuracy of a fine-tuned LLM:</p> <p>Post-training: A catch-all term for techniques applied after a fine-tuned model is produced to improve its accuracy without changing its weights.</p> <p>Supervised fine-tuning: Providing small sets of ideal input-output pairs to serve as a strong anchor for the LLM to learn from.</p> <p>Direct preference optimization: Giving the LLM multiple possible outputs but indicating which one is preferred to guide its learning.</p> <p>Reinforcement learning: Using feedback to reward the model for desired behavior.</p> <p>Prompt engineering: This involves using well-structured prompts. The speaker will discuss this in a later section of the video.</p> <p>Lookups from RAG documents: Using Retrieval-Augmented Generation (RAG) to fetch information from known, factually correct sources. The LLM uses this information in its context window to provide accurate answers, thereby controlling hallucinations and improving overall accuracy.</p> <p>Verification layers: Adding separate layers or APIs that check an LLM's response for compliance with factual accuracy or security policies. The speaker will elaborate on this in a subsequent section.</p>"},{"location":"llmops/art-1-llmops-nptel/#prompt-engineering","title":"Prompt Engineering","text":"<p>Prompt engineering, a method for improving LLM accuracy by treating prompts with the same seriousness as code.</p> <p>Prompt Engineering Concepts It emphasizes that prompts are not just unstructured text; they are powerful tools that can significantly influence an LLM's behavior. Therefore, they should be treated with the same engineering principles as code, including versioning, control, and using DevOps and CI/CD practices. A well-tuned prompt is also closely tied to the specific model it's running on, meaning the same prompt may perform differently on another fine-tuned model.</p> <p>Engineering for Prompts To manage and control prompts effectively, the video suggests using prompt templates. These templates provide a structured framework for prompts while allowing for the parameterization of specific data points. This helps control the alignment between the prompt and the backend model, preventing the use of a prompt designed for one model on another. Templates also enable enhanced security control and allow for the tracking of usage and responses for governance purposes.</p> <p>DSPy as an open-source project that provides a Domain-Specific Language (DSL) for prompts. This framework aims to add structure to prompts without sacrificing their natural language feel, allowing for better optimization and control</p> <p>Declarative - What needed to be done Procedural - How needed to be done</p> <p>Once the prompt is defined using these structures then it can organize those structure for the prompt in such a way that it is optimized prompt.</p>"},{"location":"llmops/art-1-llmops-nptel/#verification-layer","title":"Verification Layer","text":"<p> Verification Layers Purpose: The verification layers as a method to improve LLM accuracy. This is a separate layer that takes the LLM's response and checks it for compliance with factual accuracy or security policies. This layer can act as an API that invokes the LLM, processes the response, and then provides a verified output.</p> <p>Methods of Verification: There are several methods for verification, including:</p> <ol> <li>RAG Lookup: Performing a lookup against Retrieval-Augmented Generation (RAG) documents to ensure the LLM's response is factually correct.</li> <li>Critique LLM: Using a second, more powerful LLM to critique the response of the primary LLM for factual inconsistencies.</li> <li>Static Rules: Using static rules to check the format or output of the LLM's response.</li> </ol> <p>An example of a critique LLM is given, where a verifier LLM checks a primary LLM's answer against an original PDF to verify its factual accuracy</p>"},{"location":"llmops/art-1-llmops-nptel/#example-usage-of-critique-llms","title":"Example - usage of critique LLMs","text":""},{"location":"llmops/ch1intro/","title":"Large Language Model from Development to Production","text":""},{"location":"llmops/guidemodelmerging/","title":"Model Merging: Theory and Practical Implementation","text":"<p>Model merging is a powerful technique that combines the parameters of multiple trained language models to create a single model with enhanced capabilities, all without requiring GPU resources or additional training data. This approach has proven surprisingly effective, with several merged models achieving state-of-the-art performance on the Open LLM Leaderboard.[1][2][3]</p>"},{"location":"llmops/guidemodelmerging/#theoretical-foundation","title":"Theoretical Foundation","text":""},{"location":"llmops/guidemodelmerging/#core-principles","title":"Core Principles","text":"<p>Model merging operates under the assumption that fine-tuned models from a shared initialization often lie in a connected low-loss basin of the parameter space. This geometric property enables weight-space averaging to improve accuracy and robustness beyond individual models. The technique exploits the linear mode connectivity between models, where interpolated parameters maintain reasonable performance throughout the interpolation path.[2][4][5][6]</p> <p>The fundamental insight is that specialized models can be combined by directly manipulating their weights rather than through ensemble methods or additional training. This creates a unified model that inherits capabilities from multiple sources while maintaining the inference cost of a single model.[7][2]</p>"},{"location":"llmops/guidemodelmerging/#mathematical-framework","title":"Mathematical Framework","text":"<p>The basic mathematical operation underlying most merging methods is weighted parameter averaging:</p> \\[ \\theta_{merged} = \\sum_{i=1}^{N} w_i \\theta_i \\] <p>where </p> \\[ \\theta_i \\] <p>represents the parameters of model  $$ i$$, $$</p> <p>w_i $$ are the combining weights, and $$</p> <p>N$$ is the number of models. Different merging methods vary in how they compute these weights and handle parameter conflicts.[4][2]</p>"},{"location":"llmops/guidemodelmerging/#core-merging-methods","title":"Core Merging Methods","text":""},{"location":"llmops/guidemodelmerging/#linear-interpolation-and-model-soups","title":"Linear Interpolation and Model Soups","text":"<p>Linear interpolation represents the simplest merging approach, computing a weighted average of model parameters. The \"model soups\" method extends this by averaging multiple fine-tuned checkpoints from the same base model, often outperforming the best individual checkpoint.[5][6][2]</p> <p>The method supports both naive averaging (combining all models equally) and greedy selection (iteratively adding models that improve performance). Research shows that uniform averaging with weights summing to 1.0 typically produces optimal results.[1][2][5]</p>"},{"location":"llmops/guidemodelmerging/#spherical-linear-interpolation-slerp","title":"Spherical Linear Interpolation (SLERP)","text":"<p>SLERP addresses limitations of linear interpolation by preserving the geometric properties of the parameter space. Unlike linear interpolation, which can reduce parameter magnitudes in high-dimensional spaces, SLERP maintains constant rates of change and preserves directional information that often represents meaningful feature learning.[8][2][1]</p> <p>The SLERP algorithm normalizes parameter vectors to unit length, calculates angles between them using dot products, and applies trigonometric weighting factors based on the interpolation parameter \\(\\(t\\)\\). When vectors are nearly collinear, the method defaults to linear interpolation for computational efficiency.[8][1]</p>"},{"location":"llmops/guidemodelmerging/#ties-merging","title":"TIES-Merging","text":"<p>Task Interference Elimination and Sign Selection (TIES) addresses two critical challenges in model merging: parameter redundancy and sign disagreements. The method operates through three sequential steps:[9][1]</p> <ol> <li>Trim: Eliminates redundant parameters by retaining only the top-k% most significant changes (determined by the density parameter) and resetting others to zero[10][9]</li> <li>Elect Sign: Resolves conflicts where different models suggest opposing parameter adjustments by creating a unified sign vector based on the most dominant direction[10][9]</li> <li>Disjoint Merge: Averages parameter values that align with the elected sign vector, excluding zero values[9][10]</li> </ol> <p>This structured approach enables TIES to merge multiple models simultaneously while minimizing interference effects.[1][9]</p>"},{"location":"llmops/guidemodelmerging/#dare-merging","title":"DARE Merging","text":"<p>Drop And REscale (DARE) merging employs a probabilistic approach similar to TIES but with key differences. Instead of deterministic trimming, DARE randomly resets fine-tuned weights to their original base model values, then rescales the remaining weights to preserve output expectations.[2][4][1]</p> <p>Mergekit implements two DARE variants: <code>dare_linear</code> (without sign election) and <code>dare_ties</code> (incorporating TIES sign election). The density parameter controls the probability of dropping each parameter, with values around 0.53 showing empirically strong results.[7][1]</p>"},{"location":"llmops/guidemodelmerging/#fisher-information-merging","title":"Fisher Information Merging","text":"<p>Fisher merging provides a principled approach to parameter weighting based on the Fisher information matrix, which quantifies parameter importance. The method computes diagonal approximations to avoid computational complexity:[11][12]</p> \\[ F_{\\theta}[j] = E_{x \\sim p(x)} \\left[ \\left( \\frac{\\partial \\log p(y|x,\\theta)}{\\partial \\theta_j} \\right)^2 \\right] \\] <p>Parameters are then weighted according to their Fisher information values, creating informed combinations that consider parameter significance rather than treating all parameters equally.[12][11]</p>"},{"location":"llmops/guidemodelmerging/#practical-implementation","title":"Practical Implementation","text":""},{"location":"llmops/guidemodelmerging/#using-mergekit","title":"Using Mergekit","text":"<p>Mergekit serves as the primary production-ready tool for model merging, supporting multiple architectures including Llama, Mistral, GPT-NeoX, and StableLM. The library enables both CPU and GPU execution with lazy tensor loading for memory efficiency.[7][1]</p> <p>Installation requires cloning the repository and installing in editable mode:</p> <pre><code>git clone https://github.com/arcee-ai/mergekit.git\ncd mergekit\npip install -e .\n</code></pre> <p>Merging operations use YAML configuration files specifying models, methods, and parameters. The <code>mergekit-yaml</code> command processes these configurations:[7][1]</p> <pre><code>mergekit-yaml config.yaml ./output --copy-tokenizer --lazy-unpickle\n</code></pre>"},{"location":"llmops/guidemodelmerging/#configuration-examples","title":"Configuration Examples","text":"<p>SLERP configurations support layer-specific interpolation parameters, enabling fine-grained control over different model components:[1]</p> <pre><code>slices:\n  - sources:\n    - model: model1_path\n      layer_range: [0, 32]\n    - model: model2_path  \n      layer_range: [0, 32]\nmerge_method: slerp\nbase_model: model1_path\nparameters:\n  t:\n    - filter: self_attn\n      value: [0, 0.5, 0.3, 0.7, 1]\n    - filter: mlp\n      value: [1, 0.5, 0.7, 0.3, 0]\n    - value: 0.5\ndtype: bfloat16\n</code></pre> <p>TIES configurations specify density and weight parameters for each model, with automatic normalization available:[1]</p> <pre><code>models:\n  - model: base_model_path\n  - model: specialist_model_1\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: specialist_model_2\n    parameters:\n      density: 0.5\n      weight: 0.3\nmerge_method: ties\nbase_model: base_model_path\nparameters:\n  normalize: true\ndtype: float16\n</code></pre>"},{"location":"llmops/guidemodelmerging/#manual-implementation","title":"Manual Implementation","text":"<p>For custom applications, manual implementations provide full control over the merging process. Linear interpolation serves as the foundation:[4][1]</p> <pre><code>def linear_interpolation_merge(model1_state, model2_state, alpha=0.5):\n    merged_state = OrderedDict()\n    for key in model1_state.keys():\n        if key in model2_state:\n            merged_state[key] = alpha * model1_state[key] + (1 - alpha) * model2_state[key]\n        else:\n            merged_state[key] = model1_state[key]\n    return merged_state\n</code></pre> <p>SLERP implementation requires careful handling of numerical stability and edge cases:[13][8]</p> <pre><code>def slerp(v1, v2, t, epsilon=1e-7):\n    v1_norm = v1 / (torch.norm(v1, dim=-1, keepdim=True) + epsilon)\n    v2_norm = v2 / (torch.norm(v2, dim=-1, keepdim=True) + epsilon)\n\n    dot = torch.sum(v1_norm * v2_norm, dim=-1, keepdim=True)\n    dot = torch.clamp(dot, -1.0, 1.0)\n\n    theta = torch.acos(torch.abs(dot))\n    sin_theta = torch.sin(theta)\n    linear_mask = sin_theta &lt; epsilon\n\n    a = torch.sin((1 - t) * theta) / (sin_theta + epsilon)\n    b = torch.sin(t * theta) / (sin_theta + epsilon)\n\n    result = a * v1_norm + b * v2_norm\n    linear_result = (1 - t) * v1_norm + t * v2_norm\n    result = torch.where(linear_mask, linear_result, result)\n\n    return result\n</code></pre>"},{"location":"llmops/guidemodelmerging/#peft-integration","title":"PEFT Integration","text":"<p>Parameter-Efficient Fine-Tuning (PEFT) adapters can be merged with base models using the PEFT library. The process involves loading the base model, attaching the adapter, and using the <code>merge_and_unload()</code> method:[14][15][16]</p> <pre><code>from transformers import AutoModelForCausalLM\nfrom peft import PeftModel\n\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_path)\nmodel = PeftModel.from_pretrained(base_model, peft_adapter_path)\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(output_path)\n</code></pre> <p>Multiple PEFT adapters can be combined using weighted merging with methods like TIES:[14]</p> <pre><code>model.add_weighted_adapter(\n    adapters=[\"adapter1\", \"adapter2\", \"adapter3\"],\n    weights=[2.0, 1.0, 1.0],\n    adapter_name=\"merged_adapter\",\n    combination_type=\"ties\",\n    density=0.2\n)\n</code></pre>"},{"location":"llmops/guidemodelmerging/#implementation-considerations","title":"Implementation Considerations","text":""},{"location":"llmops/guidemodelmerging/#compatibility-requirements","title":"Compatibility Requirements","text":"<p>Successful merging requires models to share identical architectures, tensor shapes, and tokenizers. Mismatched vocabularies or architectures will cause failures or produce corrupted models. Pre-merge validation should verify parameter name consistency and tensor shape compatibility.[8][7][1]</p>"},{"location":"llmops/guidemodelmerging/#memory-management","title":"Memory Management","text":"<p>Large language models require careful memory management during merging. Mergekit supports lazy loading to minimize memory usage, loading tensors only when needed. The <code>--out-shard-size</code> parameter controls output file chunking, enabling processing on systems with limited RAM.[7][1]</p>"},{"location":"llmops/guidemodelmerging/#performance-optimization","title":"Performance Optimization","text":"<p>Merging performance can be optimized through several strategies:[7][1]</p> <ul> <li>Use appropriate data types (bfloat16/float16) to reduce memory footprint[1]</li> <li>Enable lazy unpickling with <code>--lazy-unpickle</code> for faster loading[1]</li> <li>Utilize GPU acceleration with <code>--cuda</code> when available[7]</li> <li>Implement chunked processing for extremely large models[7]</li> </ul>"},{"location":"llmops/guidemodelmerging/#validation-and-quality-control","title":"Validation and Quality Control","text":"<p>Merged models require thorough validation before deployment. Evaluation should include:[17][1]</p> <ul> <li>Perplexity measurements on representative datasets[17]</li> <li>Task-specific benchmarks (ARC, HellaSwag, MMLU, etc.)[17][1]</li> <li>Qualitative assessment of generated outputs[1]</li> <li>Comparison with baseline models and ensembles[1]</li> </ul>"},{"location":"llmops/guidemodelmerging/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"llmops/guidemodelmerging/#frankenmerging-and-passthrough","title":"Frankenmerging and Passthrough","text":"<p>The passthrough method enables \"Frankenmerging\" by concatenating layers from different models, creating architectures with exotic parameter counts. This experimental technique has produced impressive results like SOLAR-10.7B, which combines layers through depth-up scaling.[4][1]</p>"},{"location":"llmops/guidemodelmerging/#evolutionary-optimization","title":"Evolutionary Optimization","text":"<p>Recent work explores automated discovery of optimal merge recipes through evolutionary algorithms. These methods systematically search hyperparameter spaces to identify high-performing combinations without manual tuning.[3][18]</p>"},{"location":"llmops/guidemodelmerging/#multi-stage-merging","title":"Multi-Stage Merging","text":"<p>Complex merging workflows can be implemented through multi-stage approaches, where initial merges serve as inputs to subsequent operations. The <code>mergekit-multi</code> tool supports such workflows through unified configuration files.[7]</p>"},{"location":"llmops/guidemodelmerging/#best-practices-and-recommendations","title":"Best Practices and Recommendations","text":""},{"location":"llmops/guidemodelmerging/#model-selection","title":"Model Selection","text":"<p>Choose parent models with complementary capabilities rather than redundant skills. Models fine-tuned from the same base initialization typically merge more successfully than those with different origins. Verify compatibility through architecture inspection and small-scale testing before full merging.[5][2][1]</p>"},{"location":"llmops/guidemodelmerging/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>Systematically explore merge coefficients through grid search or validation-guided selection. For TIES and DARE methods, density parameters around 0.5-0.6 often provide good results, though optimal values vary by model combination. Weight normalization generally improves stability and performance.[2][5][1]</p>"},{"location":"llmops/guidemodelmerging/#quality-assessment","title":"Quality Assessment","text":"<p>Evaluate merged models across multiple metrics and domains to identify potential weaknesses. Monitor for catastrophic forgetting in specialized capabilities and validate that merged models maintain coherent behavior across different prompt types. Compare performance against both individual parent models and naive ensemble baselines.[17][1]</p> <p>Model merging represents a powerful paradigm for combining specialized language models into versatile, high-performing systems. Through careful selection of methods, thorough validation, and systematic hyperparameter optimization, practitioners can create merged models that exceed the capabilities of their individual components while maintaining efficient inference characteristics. As the field continues evolving, automated optimization techniques and novel merging algorithms promise to further enhance the effectiveness of this approach.</p> <p>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43</p>"},{"location":"llmops/model-merging-practical-guide/","title":"Model Merging Practical Implementation Guide","text":"<p>This guide provides hands-on code examples for implementing model merging using PyTorch and Transformers library.</p>"},{"location":"llmops/model-merging-practical-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Manual Weight Averaging</li> <li>SLERP Implementation </li> <li>TIES-Merging</li> <li>Using Mergekit</li> <li>PEFT/LoRA Merging</li> <li>Fisher Information Merging</li> <li>Best Practices</li> </ol>"},{"location":"llmops/model-merging-practical-guide/#manual-weight-averaging","title":"Manual Weight Averaging","text":""},{"location":"llmops/model-merging-practical-guide/#simple-linear-interpolation","title":"Simple Linear Interpolation","text":"<pre><code># Manual Model Weight Averaging (Simple Linear Interpolation)\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel, AutoTokenizer\nfrom collections import OrderedDict\n\ndef linear_interpolation_merge(model1_state, model2_state, alpha=0.5):\n    \"\"\"\n    Simple linear interpolation between two model state dictionaries\n    merged_weights = alpha * model1 + (1 - alpha) * model2\n    \"\"\"\n    merged_state = OrderedDict()\n\n    for key in model1_state.keys():\n        if key in model2_state:\n            merged_state[key] = alpha * model1_state[key] + (1 - alpha) * model2_state[key]\n        else:\n            merged_state[key] = model1_state[key]\n\n    return merged_state\n\n# Example usage\ndef merge_two_models(model1_path, model2_path, alpha=0.5, output_path=\"./merged_model\"):\n    \"\"\"Merge two Hugging Face models with linear interpolation\"\"\"\n\n    # Load models\n    model1 = AutoModel.from_pretrained(model1_path)\n    model2 = AutoModel.from_pretrained(model2_path)\n    tokenizer = AutoTokenizer.from_pretrained(model1_path)\n\n    # Get state dictionaries\n    state1 = model1.state_dict()\n    state2 = model2.state_dict()\n\n    # Merge weights\n    merged_state = linear_interpolation_merge(state1, state2, alpha)\n\n    # Create new model with merged weights  \n    merged_model = AutoModel.from_pretrained(model1_path)\n    merged_model.load_state_dict(merged_state)\n\n    # Save merged model\n    merged_model.save_pretrained(output_path)\n    tokenizer.save_pretrained(output_path)\n\n    return merged_model\n\n# Usage example:\n# merged_model = merge_two_models(\"model1_path\", \"model2_path\", alpha=0.3)\n</code></pre>"},{"location":"llmops/model-merging-practical-guide/#slerp-implementation","title":"SLERP Implementation","text":""},{"location":"llmops/model-merging-practical-guide/#spherical-linear-interpolation","title":"Spherical Linear Interpolation","text":"<pre><code># SLERP (Spherical Linear Interpolation) Implementation\nimport torch\nimport numpy as np\n\ndef slerp(v1, v2, t, epsilon=1e-7):\n    \"\"\"\n    Spherical linear interpolation between two tensors\n\n    Args:\n        v1, v2: Input tensors to interpolate between\n        t: Interpolation parameter (0 = v1, 1 = v2)\n        epsilon: Small value to avoid division by zero\n    \"\"\"\n    # Normalize vectors\n    v1_norm = v1 / (torch.norm(v1, dim=-1, keepdim=True) + epsilon)\n    v2_norm = v2 / (torch.norm(v2, dim=-1, keepdim=True) + epsilon)\n\n    # Compute dot product\n    dot = torch.sum(v1_norm * v2_norm, dim=-1, keepdim=True)\n\n    # Clamp dot product to avoid numerical issues\n    dot = torch.clamp(dot, -1.0, 1.0)\n\n    # Compute angle\n    theta = torch.acos(torch.abs(dot))\n\n    # Handle near-parallel vectors (fall back to linear interpolation)\n    sin_theta = torch.sin(theta)\n    linear_mask = sin_theta &lt; epsilon\n\n    # SLERP calculation\n    a = torch.sin((1 - t) * theta) / (sin_theta + epsilon)\n    b = torch.sin(t * theta) / (sin_theta + epsilon)\n\n    # Apply SLERP\n    result = a * v1_norm + b * v2_norm\n\n    # Use linear interpolation for near-parallel vectors\n    linear_result = (1 - t) * v1_norm + t * v2_norm\n    result = torch.where(linear_mask, linear_result, result)\n\n    return result\n\ndef slerp_merge_models(model1_state, model2_state, t=0.5):\n    \"\"\"Apply SLERP to all matching parameters in two models\"\"\"\n    merged_state = OrderedDict()\n\n    for key in model1_state.keys():\n        if key in model2_state and model1_state[key].shape == model2_state[key].shape:\n            # Flatten tensors for SLERP, then reshape back\n            original_shape = model1_state[key].shape\n            flat1 = model1_state[key].flatten()\n            flat2 = model2_state[key].flatten()\n\n            # Apply SLERP\n            merged_flat = slerp(flat1, flat2, t)\n            merged_state[key] = merged_flat.reshape(original_shape)\n        else:\n            merged_state[key] = model1_state[key]\n\n    return merged_state\n</code></pre>"},{"location":"llmops/model-merging-practical-guide/#ties-merging","title":"TIES-Merging","text":""},{"location":"llmops/model-merging-practical-guide/#task-interference-elimination-and-sign-selection","title":"Task Interference Elimination and Sign Selection","text":"<pre><code># TIES-Merging Implementation (Simplified)\nimport torch\nfrom collections import defaultdict\n\ndef trim_parameters(task_vector, density=0.5):\n    \"\"\"Trim small parameters, keeping only top density% by magnitude\"\"\"\n    flat_params = task_vector.flatten()\n    threshold = torch.quantile(torch.abs(flat_params), 1 - density)\n    mask = torch.abs(task_vector) &gt;= threshold\n    return task_vector * mask\n\ndef elect_sign(task_vectors):\n    \"\"\"Resolve sign conflicts by majority vote\"\"\"\n    # Stack all task vectors\n    stacked = torch.stack(task_vectors, dim=0)\n\n    # Count positive and negative signs\n    pos_count = (stacked &gt; 0).sum(dim=0)\n    neg_count = (stacked &lt; 0).sum(dim=0)\n\n    # Majority vote for signs\n    sign_mask = pos_count &gt;= neg_count\n    return sign_mask.float() * 2 - 1  # Convert to -1, 1\n\ndef disjoint_merge(task_vectors, sign_vector):\n    \"\"\"Average parameters that agree with elected sign\"\"\"\n    merged = torch.zeros_like(task_vectors[0])\n    count = torch.zeros_like(task_vectors[0])\n\n    for tv in task_vectors:\n        # Only include parameters that agree with elected sign\n        agrees = torch.sign(tv) == torch.sign(sign_vector)\n        merged += tv * agrees\n        count += agrees\n\n    # Avoid division by zero\n    count = torch.where(count == 0, torch.ones_like(count), count)\n    return merged / count\n\ndef ties_merge(base_model_state, fine_tuned_states, density=0.5):\n    \"\"\"\n    TIES merging implementation\n\n    Args:\n        base_model_state: Base model state dict\n        fine_tuned_states: List of fine-tuned model state dicts\n        density: Fraction of parameters to keep after trimming\n    \"\"\"\n    merged_state = base_model_state.copy()\n\n    for key in base_model_state.keys():\n        if all(key in state for state in fine_tuned_states):\n            # Compute task vectors (differences from base)\n            task_vectors = []\n            for state in fine_tuned_states:\n                task_vector = state[key] - base_model_state[key]\n                # Trim small parameters\n                trimmed_tv = trim_parameters(task_vector, density)\n                task_vectors.append(trimmed_tv)\n\n            if task_vectors:\n                # Elect signs and merge\n                sign_vector = elect_sign(task_vectors)\n                merged_delta = disjoint_merge(task_vectors, sign_vector)\n\n                # Add back to base model\n                merged_state[key] = base_model_state[key] + merged_delta\n\n    return merged_state\n</code></pre>"},{"location":"llmops/model-merging-practical-guide/#using-mergekit","title":"Using Mergekit","text":""},{"location":"llmops/model-merging-practical-guide/#installation-and-setup","title":"Installation and Setup","text":"<pre><code># Install mergekit\ngit clone https://github.com/arcee-ai/mergekit.git\ncd mergekit\npip install -e .\n</code></pre>"},{"location":"llmops/model-merging-practical-guide/#yaml-configuration-examples","title":"YAML Configuration Examples","text":"<pre><code># SLERP Configuration\nslices:\n  - sources:\n    - model: microsoft/DialoGPT-medium\n      layer_range: [0, 24]\n    - model: microsoft/DialoGPT-large  \n      layer_range: [0, 24]\nmerge_method: slerp\nbase_model: microsoft/DialoGPT-medium\nparameters:\n  t:\n    - filter: self_attn\n      value: [0, 0.5, 0.3, 0.7, 1]\n    - filter: mlp\n      value: [1, 0.5, 0.7, 0.3, 0]\n    - value: 0.5\ndtype: bfloat16\n</code></pre> <pre><code># TIES Configuration  \nmodels:\n  - model: microsoft/DialoGPT-medium\n    # no parameters necessary for base model\n  - model: microsoft/DialoGPT-large\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: huggingface/CodeBERTa-small-v1\n    parameters:\n      density: 0.5\n      weight: 0.3\nmerge_method: ties\nbase_model: microsoft/DialoGPT-medium\nparameters:\n  normalize: true\ndtype: float16\n</code></pre>"},{"location":"llmops/model-merging-practical-guide/#python-usage-with-mergekit","title":"Python Usage with Mergekit","text":"<pre><code>import yaml\nimport subprocess\nimport os\n\ndef run_mergekit_merge(config_dict, output_dir, config_file=\"merge_config.yaml\"):\n    \"\"\"Run mergekit merge using Python\"\"\"\n\n    # Save config to file\n    with open(config_file, 'w') as f:\n        yaml.dump(config_dict, f)\n\n    # Run mergekit command\n    cmd = [\n        \"mergekit-yaml\", \n        config_file, \n        output_dir,\n        \"--copy-tokenizer\",\n        \"--allow-crimes\", \n        \"--out-shard-size\", \"1B\",\n        \"--lazy-unpickle\"\n    ]\n\n    try:\n        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n        print(f\"Merge completed successfully! Output saved to {output_dir}\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Merge failed: {e}\")\n        print(f\"Error output: {e.stderr}\")\n        return False\n\n# Example usage\nconfig = {\n    'slices': [\n        {\n            'sources': [\n                {'model': 'microsoft/DialoGPT-medium', 'layer_range': [0, 24]},\n                {'model': 'microsoft/DialoGPT-large', 'layer_range': [0, 24]}\n            ]\n        }\n    ],\n    'merge_method': 'slerp',\n    'base_model': 'microsoft/DialoGPT-medium',\n    'parameters': {\n        't': 0.5\n    },\n    'dtype': 'bfloat16'\n}\n\n# run_mergekit_merge(config, \"./merged_model\")\n</code></pre>"},{"location":"llmops/model-merging-practical-guide/#peftlora-merging","title":"PEFT/LoRA Merging","text":""},{"location":"llmops/model-merging-practical-guide/#basic-peft-adapter-merging","title":"Basic PEFT Adapter Merging","text":"<pre><code># PEFT (LoRA/Adapter) Merging with Base Models\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel, PeftConfig\nimport torch\n\ndef merge_peft_with_base(base_model_path, peft_model_path, output_path):\n    \"\"\"\n    Merge PEFT adapter with base model\n\n    Args:\n        base_model_path: Path to base model\n        peft_model_path: Path to PEFT adapter\n        output_path: Where to save merged model\n    \"\"\"\n\n    # Load base model\n    print(\"Loading base model...\")\n    base_model = AutoModelForCausalLM.from_pretrained(\n        base_model_path,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n\n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n\n    # Load PEFT model\n    print(\"Loading PEFT adapter...\")\n    model = PeftModel.from_pretrained(base_model, peft_model_path)\n\n    # Merge and unload adapter\n    print(\"Merging adapter with base model...\")\n    merged_model = model.merge_and_unload()\n\n    # Save merged model\n    print(f\"Saving merged model to {output_path}\")\n    merged_model.save_pretrained(output_path)\n    tokenizer.save_pretrained(output_path)\n\n    return merged_model\n\n# Multiple PEFT adapters merging using PEFT library\ndef merge_multiple_peft_adapters(base_model_path, adapter_paths, weights, output_path):\n    \"\"\"\n    Merge multiple PEFT adapters with different weights\n\n    Args:\n        base_model_path: Path to base model  \n        adapter_paths: List of paths to PEFT adapters\n        weights: List of weights for each adapter\n        output_path: Where to save merged model\n    \"\"\"\n    from peft import PeftModel\n\n    # Load base model\n    base_model = AutoModelForCausalLM.from_pretrained(\n        base_model_path,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n\n    # Load all adapters\n    model = base_model\n    adapter_names = []\n\n    for i, adapter_path in enumerate(adapter_paths):\n        adapter_name = f\"adapter_{i}\"\n        model = PeftModel.from_pretrained(model, adapter_path, adapter_name=adapter_name)\n        adapter_names.append(adapter_name)\n\n    # Set up weighted merging\n    model.add_weighted_adapter(\n        adapters=adapter_names,\n        weights=weights,\n        adapter_name=\"merged_adapter\",\n        combination_type=\"ties\"  # or \"linear\", \"dare_ties\", etc.\n    )\n\n    # Set the merged adapter as active\n    model.set_adapter(\"merged_adapter\")\n\n    # Merge and save\n    merged_model = model.merge_and_unload()\n    merged_model.save_pretrained(output_path)\n\n    return merged_model\n\n# Usage example:\n# merge_peft_with_base(\"meta-llama/Llama-2-7b-hf\", \"./lora_adapter\", \"./merged_output\")\n</code></pre>"},{"location":"llmops/model-merging-practical-guide/#fisher-information-merging","title":"Fisher Information Merging","text":""},{"location":"llmops/model-merging-practical-guide/#implementation-with-fisher-information-matrix","title":"Implementation with Fisher Information Matrix","text":"<pre><code># Fisher Information Matrix Merging Implementation\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\ndef compute_fisher_information(model, dataloader, num_samples=1000):\n    \"\"\"\n    Compute diagonal Fisher information matrix for a model\n\n    Args:\n        model: PyTorch model\n        dataloader: DataLoader with labeled data\n        num_samples: Number of samples to use for Fisher computation\n    \"\"\"\n    model.eval()\n    fisher_dict = {}\n\n    # Initialize Fisher information storage\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            fisher_dict[name] = torch.zeros_like(param)\n\n    samples_processed = 0\n\n    for batch_idx, (data, target) in enumerate(dataloader):\n        if samples_processed &gt;= num_samples:\n            break\n\n        data, target = data.cuda(), target.cuda()\n        model.zero_grad()\n\n        # Forward pass\n        output = model(data)\n\n        # Compute loss and gradients\n        loss = F.cross_entropy(output, target)\n        loss.backward()\n\n        # Accumulate squared gradients (diagonal Fisher approximation)\n        for name, param in model.named_parameters():\n            if param.requires_grad and param.grad is not None:\n                fisher_dict[name] += param.grad.data ** 2\n\n        samples_processed += data.size(0)\n\n    # Normalize by number of samples\n    for name in fisher_dict:\n        fisher_dict[name] /= samples_processed\n\n    return fisher_dict\n\ndef fisher_weighted_averaging(models, fisher_matrices, normalize=True):\n    \"\"\"\n    Merge models using Fisher-weighted averaging\n\n    Args:\n        models: List of model state dictionaries\n        fisher_matrices: List of corresponding Fisher information matrices\n        normalize: Whether to normalize Fisher weights\n    \"\"\"\n    if len(models) != len(fisher_matrices):\n        raise ValueError(\"Number of models must match number of Fisher matrices\")\n\n    merged_state = {}\n\n    # Get parameter names from first model\n    param_names = list(models[0].keys())\n\n    for param_name in param_names:\n        # Collect parameters and Fisher weights\n        params = [model[param_name] for model in models]\n        fishers = [fisher[param_name] for fisher in fisher_matrices]\n\n        # Stack parameters and Fisher weights\n        stacked_params = torch.stack(params, dim=0)  # [num_models, ...param_shape]\n        stacked_fishers = torch.stack(fishers, dim=0)  # [num_models, ...param_shape]\n\n        # Normalize Fisher weights if requested\n        if normalize:\n            fisher_sum = stacked_fishers.sum(dim=0, keepdim=True)\n            fisher_sum = torch.where(fisher_sum == 0, torch.ones_like(fisher_sum), fisher_sum)\n            weights = stacked_fishers / fisher_sum\n        else:\n            weights = stacked_fishers\n\n        # Compute weighted average\n        merged_param = (weights * stacked_params).sum(dim=0)\n        merged_state[param_name] = merged_param\n\n    return merged_state\n</code></pre>"},{"location":"llmops/model-merging-practical-guide/#best-practices","title":"Best Practices","text":""},{"location":"llmops/model-merging-practical-guide/#1-model-compatibility","title":"1. Model Compatibility","text":"<pre><code>def check_model_compatibility(model1_path, model2_path):\n    \"\"\"Check if two models can be merged\"\"\"\n\n    model1 = AutoModel.from_pretrained(model1_path)\n    model2 = AutoModel.from_pretrained(model2_path)\n\n    state1 = model1.state_dict()\n    state2 = model2.state_dict()\n\n    # Check architecture compatibility\n    if set(state1.keys()) != set(state2.keys()):\n        print(\"Warning: Models have different parameter names\")\n        return False\n\n    # Check tensor shapes\n    for key in state1.keys():\n        if state1[key].shape != state2[key].shape:\n            print(f\"Warning: Shape mismatch for {key}: {state1[key].shape} vs {state2[key].shape}\")\n            return False\n\n    print(\"Models are compatible for merging!\")\n    return True\n</code></pre>"},{"location":"llmops/model-merging-practical-guide/#2-memory-efficient-merging","title":"2. Memory-Efficient Merging","text":"<pre><code>def memory_efficient_merge(model_paths, merge_fn, output_path, chunk_size=1000):\n    \"\"\"Merge models in chunks to save memory\"\"\"\n\n    import gc\n\n    # Load first model as base\n    merged_state = torch.load(model_paths[0], map_location='cpu')\n\n    for model_path in model_paths[1:]:\n        # Load model\n        model_state = torch.load(model_path, map_location='cpu')\n\n        # Merge in chunks\n        merged_state = merge_fn(merged_state, model_state)\n\n        # Clear memory\n        del model_state\n        gc.collect()\n\n    # Save merged model\n    torch.save(merged_state, output_path)\n    return merged_state\n</code></pre>"},{"location":"llmops/model-merging-practical-guide/#3-validation-and-testing","title":"3. Validation and Testing","text":"<pre><code>def validate_merged_model(merged_model_path, test_data):\n    \"\"\"Validate merged model performance\"\"\"\n\n    from transformers import pipeline\n\n    # Load merged model\n    pipe = pipeline(\"text-generation\", model=merged_model_path)\n\n    # Test on sample data\n    for prompt in test_data:\n        output = pipe(prompt, max_length=50)\n        print(f\"Input: {prompt}\")\n        print(f\"Output: {output[0]['generated_text']}\")\n        print(\"-\" * 50)\n</code></pre>"},{"location":"llmops/model-merging-practical-guide/#4-configuration-management","title":"4. Configuration Management","text":"<pre><code>import json\nfrom datetime import datetime\n\ndef save_merge_config(config, output_path):\n    \"\"\"Save merge configuration for reproducibility\"\"\"\n\n    config['timestamp'] = datetime.now().isoformat()\n    config['mergekit_version'] = \"0.4.2\"  # Update with actual version\n\n    with open(f\"{output_path}/merge_config.json\", 'w') as f:\n        json.dump(config, f, indent=2)\n\ndef load_merge_config(config_path):\n    \"\"\"Load previously saved merge configuration\"\"\"\n\n    with open(config_path, 'r') as f:\n        return json.load(f)\n</code></pre>"},{"location":"llmops/model-merging-practical-guide/#command-line-usage","title":"Command Line Usage","text":""},{"location":"llmops/model-merging-practical-guide/#basic-mergekit-commands","title":"Basic Mergekit Commands","text":"<pre><code># SLERP merge\nmergekit-yaml slerp_config.yaml ./slerp_output --copy-tokenizer --lazy-unpickle\n\n# TIES merge  \nmergekit-yaml ties_config.yaml ./ties_output --copy-tokenizer --allow-crimes\n\n# DARE merge\nmergekit-yaml dare_config.yaml ./dare_output --cuda --copy-tokenizer\n\n# With custom options\nmergekit-yaml config.yaml ./output \\\\\n    --copy-tokenizer \\\\\n    --allow-crimes \\\\\n    --out-shard-size 1B \\\\\n    --lazy-unpickle \\\\\n    --trust-remote-code\n</code></pre>"},{"location":"llmops/model-merging-practical-guide/#uploading-to-hugging-face-hub","title":"Uploading to Hugging Face Hub","text":"<pre><code># Login to Hugging Face\nhuggingface-cli login\n\n# Upload merged model\nhuggingface-cli upload username/merged-model-name ./merged_output .\n</code></pre> <p>This guide provides comprehensive examples for implementing model merging in practice. Choose the appropriate method based on your specific requirements:</p> <ul> <li>Linear interpolation: Simple, fast, good for similar models</li> <li>SLERP: Better geometric properties, preserves model structure  </li> <li>TIES: Best for merging multiple task-specific models</li> <li>DARE: Robust to interference, good empirical results</li> <li>Fisher merging: Principled approach using parameter importance</li> <li>Mergekit: Production-ready tool with many advanced methods</li> </ul> <p>Remember to always validate merged models thoroughly before deployment!</p>"},{"location":"llmops/modelmerging/","title":"LLM : Model Merging/Model Fusion","text":"<ul> <li>Combines the parameters of two or more compatible LLMs to produce a single model</li> <li>No additional gradient-based training</li> <li>Enabling low-cost</li> <li>CPU-only creation of new variants that have, in several cases, reached state-of-the-art scores on the Open LLM Leaderboard.</li> <li>data-free, fast to iterate, and especially effective when source models contribute complementary capabilities while sharing a common architecture and tokenizer.</li> </ul> <p>What is model merging ?</p> <p>How model merging is done ?</p> <ul> <li>It algebraically mixes checkpoints\u2014often as full weights or fine-tuning deltas\u2014so the merged model inherits behaviors from multiple specialized models without re-training on raw data.</li> <li>It done offline via weight-space operations like linear averaging, spherical interpolation, or structured selection, and can run entirely on CPU with libraries such as mergekit.</li> </ul> <p>Why does model merging works ?</p> <ul> <li>The fine-tuned models from a shared initialization often lie in a connected low-loss basin, so averaging their weights can improve accuracy and robustness\u2014an effect popularized as \u201cmodel soups\u201d.</li> <li>The main failure mode is parameter interference (e.g., redundant updates and sign conflicts across models), which specialized schemes like TIES address by trimming small deltas and resolving sign disagreements before merging.</li> <li>Thus, well designed merging can generalize better than naive averages and provide stronger starting points for subsequent fine-tuning.</li> </ul> <p>What are the most common methods for it ?</p> <p>Linear/Soup Averaging :</p>"},{"location":"mlops/ch1intro/","title":"Machine Learning - From Developement to Deployment","text":""},{"location":"mlops/deployement/art55-flask-docker-hf-ml-model/","title":"Deployment","text":"<p>docker</p>"},{"location":"multimodel/multi-model/","title":"Multi model","text":"Multi-Model Learning"},{"location":"multimodel/multi-model/#table-of-contents","title":"Table of contents","text":"<ol> <li>Image to Text Mapping </li> <li>Text to Image Mapping</li> <li>Image Text Transformer Pretraining</li> <li>Vision LLMs</li> </ol> <p>What is multi-model learning ?</p> <p>Mapping Image to Image then CNN itself can learn the mapping. Mapping Text to Text then RNN itself can learn the mapping. What is the possible architecture for learning the Image to text mapping ?</p> <p>Images are suitable for the CNN and while Texts are for RNN Therefore if we consider an encoder-decoder based architecture then we needed to connect the CNN + RNN such that they learning the Image to text mapping.</p>"}]}